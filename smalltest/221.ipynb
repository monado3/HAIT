{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小テスト(全3問)  \n",
    "## 第1問  重回帰\n",
    "教材ではボストンデータセットについて、２つの特徴量LSTATとRMを用いて住宅価格の中央値MEDVを予測しましたが、精度があまり良くありませんでした。そこで、特徴量についてもう一つ追加して改めて中央値を予測してみることにします。  \n",
    "それぞれの特徴量を標準化し重回帰分析にかけることで標準化偏回帰係数を求め、その大きさの評価から**DIS**が抑制関数としてMEDVに対する影響力が大きいことが判明しました。そこで、DISを特徴量に追加して再び重回帰分析してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# BostonDatasetの読み込み\n",
    "from sklearn.datasets import load_boston \n",
    "boston = load_boston()        \n",
    "\n",
    "df_data = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df_target = pd.DataFrame(boston.target, columns=['MEDV'])\n",
    "df=pd.concat([df_data,df_target], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)   \n",
    "Xにdfから\"LSTAT\",\"RM\",\"DIS\"の値,yにdfから\"MEDV\"の値を取り出してarray型で代入してください。  \n",
    "その後、ホールド・アウト法によってデータを分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#埋めてください\n",
    "X = df.loc[:,['LSTAT','RM','DIS']].values\n",
    "y = df.loc[:,['MEDV']].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test \\\n",
    "= train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)  \n",
    "線形回帰のインスタンスを生成し、モデルの学習をさせ、trainデータとtestデータの決定係数を出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2\n",
      "train:0.659\n",
      "test :0.616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#ここを埋めてください\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('R^2')\n",
    "print(f'train:{lr.score(X_train, y_train):.3f}')\n",
    "print(f'test :{lr.score(X_test , y_test ):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "特徴量に２つLSTATとRMだけを用いた場合、  \n",
    "train: 0.651  \n",
    "test : 0.607  \n",
    "という結果となりましたので、比較してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)\n",
    " RMSE（Root Mean Square Error: 平均二乗誤差の平方根）を出してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "train:5.378\n",
      "test :5.654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#ここを埋めてください\n",
    "print('RMSE')\n",
    "print(f'train:{mse(y_train, lr.predict(X_train))**(1/2):.3f}')\n",
    "print(f'test :{mse(y_test , lr.predict(X_test ))**(1/2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "RMSEとして出したこの値が意味することは何か説明してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'そのモデルについて予測が正解からどれだけズレるかを表している。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"そのモデルについて予測が正解からどれだけズレるかを表している。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\n",
    "特徴量を一つ追加してもなぜ精度があまり高くならないか要因として考えられるものをいくつか挙げ説明してください。  \n",
    "またその原因を除くために何をすればよいか挙げてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n・多重共線性\\n多重共線性とは相関が強い説明変数同士を一緒に利用したために､回帰式が信頼できないものになってしまうこと\\nこれは予測式が収束しなかったり、予測式の信頼性を低下させるので、互いに相関が強い説明変数はひとつのみ\\n使用することがよい。\\n・交互作用\\n交互作用とは、とある説明変数の効果が､別の説明変数の値によって変化する作用のこと。\\n線形モデルは構造上､交互作用を表現することができないので、予測式の構造自体を変化させる必要がある。\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "・多重共線性\n",
    "多重共線性とは相関が強い説明変数同士を一緒に利用したために､回帰式が信頼できないものになってしまうこと\n",
    "これは予測式が収束しなかったり、予測式の信頼性を低下させるので、互いに相関が強い説明変数はひとつのみ\n",
    "使用することがよい。\n",
    "・交互作用\n",
    "交互作用とは、とある説明変数の効果が､別の説明変数の値によって変化する作用のこと。\n",
    "線形モデルは構造上､交互作用を表現することができないので、予測式の構造自体を変化させる必要がある。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2問 多項式回帰   \n",
    "多項式回帰は名前の通り一つの特徴量に関して、複数の次数の基底を用意することに特徴があります。  \n",
    "一般に回帰に関してどのモデルを選ぶか考える時、目的変数と特徴量の関係を図示化をします。では多項式回帰を選ぶ場合、特徴量と目的変数の関係はどのようになっている場合でしょうか。  \n",
    "また、次数を大きくした場合どのようなデメリットがあるのか説明してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n図示したときに、特徴量と目的変数による点の集合に当てはまりそうなグラフの形が、\\n線形ではなく、多項式グラフ（1次より高次な関数）の形に近いときに、多項式回帰を選んだほうが良い\\n\\n次数を多くした場合\\ntrainデータに柔軟にフィットしすぎて過学習してしまう可能性がある。\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "図示したときに、特徴量と目的変数による点の集合に当てはまりそうなグラフの形が、\n",
    "線形ではなく、多項式グラフ（1次より高次な関数）の形に近いときに、多項式回帰を選んだほうが良い\n",
    "\n",
    "次数を多くした場合\n",
    "trainデータに柔軟にフィットしすぎて過学習してしまう可能性がある。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3問 正則化   \n",
    "HAIT教材では正則化の手法について  \n",
    "**Ridge回帰**  \n",
    "**LASSO**  \n",
    "**ElasticNet**    \n",
    "の3手法について扱いました。  \n",
    "<img src=\"https://s3-ap-northeast-1.amazonaws.com/ai-std/001.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の３つの図が3手法のいずれに該当するか答え、それぞれどのような特徴と使い方をするのか答えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n① LASSO\\n特徴選択による次元削減を自動的に行えることが特徴で、\\nサンプル数に対して特徴量が多すぎるときなどに使う。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "① LASSO\n",
    "特徴選択による次元削減を自動的に行えることが特徴で、\n",
    "サンプル数に対して特徴量が多すぎるときなどに使う。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n②Ridge回帰\\nパラメータを小さくしようという力を働かせるのに特化して過学習を防ぐという特徴\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "②Ridge回帰\n",
    "パラメータを小さくしようという力を働かせるのに特化して過学習を防ぐという特徴\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n③ElasticNet\\n１と2の中間の正則化で、適切にパラメータ調整によりちょうどいい正則化をかけれる特徴\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "③ElasticNet\n",
    "１と2の中間の正則化で、適切にパラメータ調整によりちょうどいい正則化をかけれる特徴\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
